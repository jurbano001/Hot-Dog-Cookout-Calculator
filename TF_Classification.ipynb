{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jurbano001/Hot-Dog-Cookout-Calculator/blob/main/TF_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "EhkqQAiHS7Y2",
        "outputId": "e8b4636c-5059-49c6-8e42-cc35e5ff26e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
              "7024     p         x           s         n       f    y               f   \n",
              "971      e         x           f         n       t    n               f   \n",
              "5692     e         k           y         p       t    n               f   \n",
              "1907     e         x           f         n       t    n               f   \n",
              "7373     e         b           s         g       f    n               f   \n",
              "\n",
              "     gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
              "7024            c         n          b  ...                        k   \n",
              "971             c         b          n  ...                        s   \n",
              "5692            c         b          w  ...                        s   \n",
              "1907            c         b          w  ...                        s   \n",
              "7373            w         b          p  ...                        k   \n",
              "\n",
              "     stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
              "7024                      w                      p         p          w   \n",
              "971                       p                      w         p          w   \n",
              "5692                      w                      w         p          w   \n",
              "1907                      w                      p         p          w   \n",
              "7373                      w                      w         p          w   \n",
              "\n",
              "     ring-number ring-type spore-print-color population habitat  \n",
              "7024           o         e                 w          v       p  \n",
              "971            o         p                 k          v       d  \n",
              "5692           t         e                 w          c       w  \n",
              "1907           o         p                 n          v       d  \n",
              "7373           t         p                 w          s       g  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6aa1f307-380a-411c-90f7-7883444f15a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>cap-shape</th>\n",
              "      <th>cap-surface</th>\n",
              "      <th>cap-color</th>\n",
              "      <th>bruises</th>\n",
              "      <th>odor</th>\n",
              "      <th>gill-attachment</th>\n",
              "      <th>gill-spacing</th>\n",
              "      <th>gill-size</th>\n",
              "      <th>gill-color</th>\n",
              "      <th>...</th>\n",
              "      <th>stalk-surface-below-ring</th>\n",
              "      <th>stalk-color-above-ring</th>\n",
              "      <th>stalk-color-below-ring</th>\n",
              "      <th>veil-type</th>\n",
              "      <th>veil-color</th>\n",
              "      <th>ring-number</th>\n",
              "      <th>ring-type</th>\n",
              "      <th>spore-print-color</th>\n",
              "      <th>population</th>\n",
              "      <th>habitat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7024</th>\n",
              "      <td>p</td>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>y</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>b</td>\n",
              "      <td>...</td>\n",
              "      <td>k</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>e</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>n</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>v</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5692</th>\n",
              "      <td>e</td>\n",
              "      <td>k</td>\n",
              "      <td>y</td>\n",
              "      <td>p</td>\n",
              "      <td>t</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>w</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>e</td>\n",
              "      <td>w</td>\n",
              "      <td>c</td>\n",
              "      <td>w</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1907</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>w</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>v</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7373</th>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "      <td>s</td>\n",
              "      <td>g</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>w</td>\n",
              "      <td>b</td>\n",
              "      <td>p</td>\n",
              "      <td>...</td>\n",
              "      <td>k</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>s</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 23 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6aa1f307-380a-411c-90f7-7883444f15a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6aa1f307-380a-411c-90f7-7883444f15a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6aa1f307-380a-411c-90f7-7883444f15a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "url = 'https://raw.githubusercontent.com/fenago/datasets/main/mushrooms.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded = pd.get_dummies(df, drop_first=True)"
      ],
      "metadata": {
        "id": "JYV4Hf_xTDLU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_encoded.drop('class_p', axis=1)\n",
        "y = df_encoded['class_p']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4eOi7WeqTEMq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKhLqWMYTKng",
        "outputId": "a89c188b-2c2b-4656-b43f-6773642dbacc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 3s 2ms/step - loss: 0.0724 - accuracy: 0.9782\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.6340e-04 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.3812e-04 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.8113e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.1951e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 2.1075e-05 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4422e-05 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0315e-05 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.4390e-06 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.4724e-06 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.0357e-06 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0575e-06 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.3187e-06 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7900e-06 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3881e-06 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1090e-06 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.9649e-07 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.2350e-07 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.9377e-07 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.8711e-07 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.0217e-07 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3743e-07 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.8087e-07 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.3587e-07 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.0006e-07 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7109e-07 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4703e-07 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2563e-07 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0890e-07 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 9.4611e-08 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 8.1864e-08 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 7.1503e-08 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.2428e-08 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.5323e-08 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.8646e-08 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.2786e-08 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.7745e-08 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3467e-08 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9873e-08 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.6711e-08 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.3724e-08 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.1116e-08 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9161e-08 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7106e-08 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5434e-08 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3862e-08 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2521e-08 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1364e-08 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0299e-08 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.4222e-09 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.5059e-09 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.7750e-09 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.0651e-09 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.5546e-09 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.9532e-09 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.4771e-09 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.0271e-09 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.6159e-09 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.2527e-09 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.9120e-09 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.5951e-09 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3337e-09 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0560e-09 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.8443e-09 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.6296e-09 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.4473e-09 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.2630e-09 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.0990e-09 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9547e-09 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.8077e-09 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6840e-09 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.5848e-09 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4893e-09 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3877e-09 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3168e-09 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2188e-09 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1469e-09 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0837e-09 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0289e-09 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.5204e-10 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.9424e-10 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.3952e-10 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 8.0938e-10 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 7.4523e-10 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 7.0540e-10 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.5787e-10 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.2706e-10 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.0280e-10 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.7684e-10 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.3806e-10 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.1257e-10 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.0576e-10 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.7245e-10 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.4959e-10 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.3623e-10 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.3514e-10 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.1305e-10 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.0890e-10 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.8362e-10 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.8261e-10 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kJQgWvcUAUx",
        "outputId": "ce9ec07a-3590-49d1-82c0-db696caf34d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_classes_class = [\n",
        "    'p' if prob > 0.5 else 'e' for prob in np.ravel(predictions)\n",
        "]\n",
        "# convert prediction probabilities to binary predictions\n",
        "prediction_classes = [\n",
        "    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n",
        "]\n",
        "\n",
        "prediction_classes\n"
      ],
      "metadata": {
        "id": "-0TaqIPRUEOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1097449-f1b4-4b6e-c984-7bdaa44dea75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, prediction_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffFhuAPbUHsL",
        "outputId": "bdfbf4c5-84a9-44e3-eecd-853218057c6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[843   0]\n",
            " [  0 782]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_test, prediction_classes):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGZJ55L4VA2T",
        "outputId": "0ff18283-5d6a-49d9-ccd9-436fb7294295"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.0001), # decreasing learning rate\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i2j8iGCXH6l",
        "outputId": "f2662c26-dcd5-4725-9e24-9528748fbea9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 0.0765 - accuracy: 0.9760\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.7893e-04 - accuracy: 0.9998\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1412e-04 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.4481e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0556e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9296e-05 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2942e-05 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.9270e-06 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.4665e-06 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.8485e-06 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.6404e-06 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.5466e-06 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.7591e-06 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.2371e-06 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 8.9333e-07 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.7139e-07 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.1684e-07 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.0164e-07 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.2046e-07 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.5643e-07 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.0990e-07 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7368e-07 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4441e-07 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2227e-07 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0365e-07 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.9559e-08 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.6568e-08 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.6500e-08 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.8183e-08 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.0319e-08 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.4037e-08 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.8600e-08 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.4163e-08 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0102e-08 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.6633e-08 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.3833e-08 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.1188e-08 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.8949e-08 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 1.7001e-08 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.5300e-08 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.3693e-08 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.2332e-08 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.1183e-08 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0105e-08 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.1966e-09 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.3617e-09 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.5916e-09 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.9155e-09 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.3359e-09 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.8320e-09 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.3152e-09 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.8763e-09 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.4743e-09 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.1586e-09 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.8192e-09 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5116e-09 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.2514e-09 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0167e-09 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.8023e-09 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.5968e-09 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.4284e-09 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.2508e-09 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.0868e-09 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9624e-09 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.8241e-09 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7153e-09 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.6013e-09 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.5122e-09 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.4170e-09 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.3220e-09 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.2566e-09 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1762e-09 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1144e-09 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0497e-09 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.8729e-10 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.2966e-10 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.7481e-10 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.3714e-10 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.9484e-10 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.4572e-10 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.0890e-10 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.8253e-10 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.5737e-10 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.1184e-10 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.8686e-10 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.5058e-10 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.3409e-10 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.1532e-10 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.9091e-10 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.6844e-10 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.3871e-10 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.4833e-10 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.2713e-10 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.0663e-10 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.0314e-10 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.9831e-10 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.8273e-10 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.6127e-10 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5671e-10 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5640e-10 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'), # adding another layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE5Z-Vb2XTMF",
        "outputId": "e496f864-9988-4246-f8f5-e8fdb01d7cba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 2ms/step - loss: 0.0593 - accuracy: 0.9838\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 7.6498e-04 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7735e-04 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.7259e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1455e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 8.2673e-06 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.1871e-06 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.7810e-06 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.7723e-06 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.0341e-06 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.4764e-06 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 2.0608e-06 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7188e-06 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.4434e-06 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.2220e-06 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 1.0466e-06 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 8.9994e-07 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 7.7422e-07 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.6865e-07 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.7904e-07 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.0574e-07 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.4045e-07 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.8418e-07 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3598e-07 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.9430e-07 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.5890e-07 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.2746e-07 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9991e-07 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.7640e-07 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5522e-07 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3672e-07 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.2052e-07 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.0636e-07 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.4058e-08 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 8.2983e-08 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.3345e-08 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 6.4914e-08 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 5.7503e-08 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 5.1029e-08 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 4.5342e-08 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 3.9953e-08 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 3.5445e-08 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.1453e-08 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.7861e-08 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 2.4690e-08 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.1924e-08 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.9465e-08 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7385e-08 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5464e-08 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3834e-08 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2333e-08 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1069e-08 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.8880e-09 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 8.8630e-09 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 7.9364e-09 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.1442e-09 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.4398e-09 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.7758e-09 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.1906e-09 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.6975e-09 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.2074e-09 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.8375e-09 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 3.4495e-09 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 3.1293e-09 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.8457e-09 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.5806e-09 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.3502e-09 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.1388e-09 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.9568e-09 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.7718e-09 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.6156e-09 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5024e-09 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3792e-09 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.2634e-09 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.1756e-09 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0614e-09 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.7462e-10 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.9822e-10 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.2055e-10 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.6278e-10 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 7.0054e-10 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 6.5315e-10 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.1244e-10 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 5.6366e-10 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.2419e-10 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.8954e-10 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.5175e-10 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.2618e-10 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 4.0910e-10 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 3.8385e-10 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.6220e-10 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.6530e-10 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.3414e-10 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.1747e-10 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.2177e-10 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.0480e-10 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9273e-10 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 2.8637e-10 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 2.7740e-10 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 2.6614e-10 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "model_l1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_l1.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
        "    ]\n",
        ")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_l1 = model_l1.fit(\n",
        "    X_train, y_train, \n",
        "    epochs=100, \n",
        "    validation_data=(X_test, y_test), \n",
        "    callbacks=[early_stop]\n",
        ")"
      ],
      "metadata": {
        "id": "ybbHlnmyYQKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9874b839-cd7e-417e-93f7-12d28580f419"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 4ms/step - loss: 1.2330 - accuracy: 0.9677 - val_loss: 0.3981 - val_accuracy: 0.9982\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.2799 - accuracy: 0.9977 - val_loss: 0.2085 - val_accuracy: 0.9988\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1722 - accuracy: 0.9983 - val_loss: 0.1429 - val_accuracy: 0.9994\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1256 - accuracy: 0.9988 - val_loss: 0.1089 - val_accuracy: 0.9994\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0984 - accuracy: 0.9992 - val_loss: 0.0890 - val_accuracy: 0.9994\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0814 - accuracy: 0.9992 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0699 - accuracy: 0.9995 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0612 - accuracy: 0.9995 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0492 - accuracy: 0.9997 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0286 - accuracy: 0.9998 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9998 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dropout(0.9),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dropout(0.9),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dropout(0.9),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
        "    ]\n",
        ")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stop])\n"
      ],
      "metadata": {
        "id": "-kPGZDJYgQwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "3Pha5JEFhNx8",
        "outputId": "5097c9e5-9295-47dd-db6c-43c2dd297833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mushroom_classification_model.h5')"
      ],
      "metadata": {
        "id": "qhzmoJ9nkoQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.DataFrame({\n",
        "    'cap-shape': ['f'], \n",
        "    'cap-surface': ['y'], \n",
        "    'cap-color': ['n'], \n",
        "    'bruises': ['f'], \n",
        "    'odor': ['a'], \n",
        "    'gill-attachment': ['f'], \n",
        "    'gill-spacing': ['c'], \n",
        "    'gill-size': ['n'], \n",
        "    'gill-color': ['b'], \n",
        "    'stalk-shape': ['e'], \n",
        "    'stalk-root': ['c'], \n",
        "    'stalk-surface-above-ring': ['s'], \n",
        "    'stalk-surface-below-ring': ['s'], \n",
        "    'stalk-color-above-ring': ['w'], \n",
        "    'stalk-color-below-ring': ['w'], \n",
        "    'veil-type': ['p'], \n",
        "    'veil-color': ['w'], \n",
        "    'ring-number': ['o'], \n",
        "    'ring-type': ['p'], \n",
        "    'spore-print-color': ['n'], \n",
        "    'population': ['c'], \n",
        "    'habitat': ['l']\n",
        "})"
      ],
      "metadata": {
        "id": "dMAsHeySkoNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_encoded = pd.get_dummies(new_data)\n",
        "new_data_encoded = new_data_encoded.reindex(columns=X.columns, fill_value=0)"
      ],
      "metadata": {
        "id": "m33I0xIDkoKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('mushroom_classification_model.h5')\n",
        "prediction = loaded_model.predict(new_data_encoded)\n",
        "prediction_class = int(prediction > 0.5)\n",
        "\n",
        "print('Prediction:', prediction_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdEM5kB3koH4",
        "outputId": "d437be60-a25f-4f5a-862b-dde2db911e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 78ms/step\n",
            "Prediction: 1\n"
          ]
        }
      ]
    }
  ]
}